--- /opt/vllm-ascend/vllm-ascend/vllm_ascend/attention.py	2025-04-02 11:37:05.000000000 +0000
+++ /opt/mis/mis/patch/attention_forward_patch.py	2025-04-02 11:54:50.478815951 +0000
@@ -547,7 +547,7 @@
                              dtype=query.dtype,
                              device=query.device)
 
-        if hasattr(layer,'quant_method'):
+        if hasattr(layer, 'quant_method') and False: # Attention weight quantization is not supported yet.
             isPrefill = True if attn_metadata.num_prefills > 0 else False
             self.seq_lens_tensor_cpu = torch.from_numpy(np.array(attn_metadata.prefill_metadata.seq_lens).astype(np.int32)) if isPrefill \
                 else torch.from_numpy(np.array(attn_metadata.decode_metadata.seq_lens).astype(np.int32))
