--- /opt/vllm-ascend/vllm/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py	2025-05-26 16:39:18.000000000 +0000
+++ compressed_tensors_w8a8_int8.py	2025-05-28 11:44:04.000000000 +0000
@@ -3,6 +3,8 @@
 from typing import Callable, List, Optional, Set
 
 import torch
+import torch_npu
+from torch.nn import Parameter
 from compressed_tensors.quantization import QuantizationStrategy
 
 from vllm.logger import init_logger
@@ -14,6 +16,7 @@
                                            ChannelQuantScaleParameter,
                                            ModelWeightParameter,
                                            PerTensorScaleParameter)
+from vllm.model_executor.layers.quantization.utils.w8a8_utils import convert_to_channelwise
 
 logger = init_logger(__name__)
 
@@ -26,31 +29,48 @@
         self.strategy = strategy
         self.is_static_input_scheme = is_static_input_scheme
         self.input_symmetric = input_symmetric
+        self.logical_widths = None
 
     @classmethod
     def get_min_capability(cls) -> int:
         # turing and up
         return 75
 
+    @staticmethod
+    def apply_weights(layer: torch.nn.Module, x: torch.Tensor,
+                      bias: Optional[torch.Tensor]) -> torch.Tensor:
+        if not isinstance(layer, torch.nn.Module):
+            logger.error("layer must be an instance of torch.nn.Module")
+            raise TypeError("layer must be an instance of torch.nn.Module")
+        if not hasattr(layer, "qweight") and not hasattr(layer, "weight_scale"):
+            logger.error("layer must have qweight or weight_scale attributes")
+            raise AttributeError("layer must have qweight or weight_scale attributes")
+        if not isinstance(x, torch.Tensor):
+            logger.error("x must be an instance of torch.Tensor")
+            raise TypeError("x must be an instance of torch.Tensor")
+        if bias is not None and not isinstance(bias, torch.Tensor):
+            logger.error("bias must be an instance of torch.Tensor")
+            raise TypeError("bias must be an instance of torch.Tensor")
+   
+        act_dtype = x.dtype
+        ori_shape = x.shape 
+        x = x.view(-1, x.shape[-1])
+   
+        x, pertoken_scale = torch_npu.npu_dynamic_quant(x)
+   
+        output = torch_npu.npu_quant_matmul(x, layer.weight, layer.weight_scale.view(-1), offset=None,
+                                            pertoken_scale=pertoken_scale.view(-1), bias=None, output_dtype=act_dtype)
+        output = output if len(ori_shape) == 2 else output.view(ori_shape[0], ori_shape[1], output.shape[-1])
+        if bias is not None:
+            output += bias
+        return output
+
     def create_weights(self, layer: torch.nn.Module,
                        output_partition_sizes: List[int],
                        input_size_per_partition: int,
                        params_dtype: torch.dtype, weight_loader: Callable,
                        **kwargs):
-        layer.logical_widths = output_partition_sizes
-
-        scaled_mm_linear_kernel_config = ScaledMMLinearLayerConfig(
-            is_channelwise=(self.strategy == QuantizationStrategy.CHANNEL),
-            is_static_input_scheme=self.is_static_input_scheme,
-            input_symmetric=self.input_symmetric)
-
-        kernel_type = choose_scaled_mm_linear_kernel(
-            scaled_mm_linear_kernel_config)
-
-        if kernel_type.__name__ not in self._kernel_backends_being_used:
-            logger.info("Using %s for CompressedTensorsW8A8Int8",
-                        kernel_type.__name__)
-            self._kernel_backends_being_used.add(kernel_type.__name__)
+        self.logical_widths = output_partition_sizes
 
         # WEIGHT
         weight = ModelWeightParameter(data=torch.empty(
@@ -67,44 +87,41 @@
         if self.strategy == QuantizationStrategy.CHANNEL:
             weight_scale = ChannelQuantScaleParameter(
                 data=torch.empty((sum(output_partition_sizes), 1),
-                                 dtype=torch.float32),
+                                 dtype=torch.float32 if params_dtype == torch.float16 else torch.bfloat16),
                 output_dim=0,
                 weight_loader=weight_loader)
         else:
             assert self.strategy == QuantizationStrategy.TENSOR
             weight_scale = PerTensorScaleParameter(data=torch.empty(
-                len(output_partition_sizes), dtype=torch.float32),
+                len(output_partition_sizes), 
+                len(output_partition_sizes),
+                dtype=torch.float32 if params_dtype == torch.float16 else torch.bfloat16),
                                                    weight_loader=weight_loader)
         layer.register_parameter("weight_scale", weight_scale)
 
         # INPUT SCALE
         if self.is_static_input_scheme:
             input_scale = BasevLLMParameter(data=torch.empty(
-                1, dtype=torch.float32),
+                1, dtype=torch.float32 if params_dtype == torch.float16 else torch.bfloat16),
                                             weight_loader=weight_loader)
             layer.register_parameter("input_scale", input_scale)
 
-            if not self.input_symmetric:
-                # Note: compressed-tensors stores the zp using the same dtype
-                # as the weights
-                # AZP loaded as int8 but used as int32
-                input_zero_point = BasevLLMParameter(
-                    data=torch.empty(1, dtype=torch.int8),
-                    weight_loader=weight_loader)
-                layer.register_parameter("input_zero_point", input_zero_point)
-
-        self.kernel = kernel_type(c=scaled_mm_linear_kernel_config,
-                                  w_q_param_name="weight",
-                                  w_s_param_name="weight_scale",
-                                  i_s_param_name="input_scale",
-                                  i_zp_param_name="input_zero_point",
-                                  azp_adj_param_name="azp_adj")
-
     # Checkpoints are serialized in compressed-tensors format, which is
     # different from the format the kernel may want. Handle repacking here.
     def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
-        self.kernel.process_weights_after_loading(layer)
+        weight = layer.weight
+        layer.weight = Parameter(weight.t().contiguous(), requires_grad=False)
+
+        is_fused = len(self.logical_widths) > 1
+        if is_fused and self.strategy == QuantizationStrategy.TENSOR:
+            ws_channel = convert_to_channelwise(layer.weight_scale, self.logical_widths)
+            layer.weight_scale = Parameter(ws_channel, requires_grad=False)
+        else:
+            layer.weight_scale = Parameter(layer.weight_scale.data, requires_grad=False)
+
+        if self.is_static_input_scheme:
+            layer.input_scale = Parameter(layer.input_scale.max(), requires_grad=False)
+        else:
+            layer.input_scale = None
+
 
-    def apply_weights(self, layer: torch.nn.Module, x: torch.Tensor,
-                      bias: Optional[torch.Tensor]) -> torch.Tensor:
-        return self.kernel.apply_weights(layer, x, bias)
