--- /usr/local/lib/python3.11/dist-packages/vllm/entrypoints/openai/serving_chat.py	2025-08-12 12:32:09.724881152 +0000
+++ serving_chat.py	2025-08-12 13:03:55.444881152 +0000
@@ -47,6 +47,17 @@
 
 logger = init_logger(__name__)
 
+ENABLE_MIS_GUIDED_DECODE = True
+try:
+    from mis.model_executor import (
+            get_local_xgrammar_npu_guided_decoding_logits_processor, xgr_installed,
+            enable_mis_xgrammar)
+    if not enable_mis_xgrammar:
+        ENABLE_MIS_GUIDED_DECODE = False
+except ImportError as e:
+    logger.warning(f"unable to load mis guided decode params: {e}")
+    ENABLE_MIS_GUIDED_DECODE = False
+
 
 class OpenAIServingChat(OpenAIServing):
 
@@ -252,6 +263,24 @@
                         max_tokens, self.model_config.logits_processor_pattern,
                         self.default_sampling_params)
 
+                # xgrammar adapted to npu
+                if sampling_params.guided_decoding is not None and ENABLE_MIS_GUIDED_DECODE:
+                    logger.info("enable mis guided decode")
+
+                    processor = await get_local_xgrammar_npu_guided_decoding_logits_processor(
+                        guided_params=sampling_params.guided_decoding,
+                        tokenizer=tokenizer,
+                        model_config=self.model_config
+                    )
+
+                    if processor and sampling_params.logits_processors is None:
+                        sampling_params.logits_processors = []
+                    if processor:
+                        sampling_params.logits_processors.append(processor)
+                    
+                    # Unset guided decoding params after constructing the lp from them
+                    sampling_params.guided_decoding = None
+                
                 self._log_inputs(request_id,
                                  request_prompts[i],
                                  params=sampling_params,
