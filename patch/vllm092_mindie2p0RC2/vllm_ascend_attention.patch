--- /usr/local/lib/python3.11/dist-packages/vllm_ascend/attention/attention.py	2025-07-29 01:31:59.000000000 +0000
+++ attention.py	2025-07-29 06:06:09.231804416 +0000
@@ -704,7 +704,7 @@
                 self.key_cache, self.value_cache = kv_cache[0], kv_cache[1]
             slots = attn_metadata.slot_mapping
 
-        if hasattr(layer, 'quant_method'):
+        if hasattr(layer, 'quant_method') and False:  # Attention weight quantization is not supported yet.
             isPrefill = True if attn_metadata.num_prefills > 0 else False
             if isPrefill:
                 assert attn_metadata.prefill_metadata is not None
