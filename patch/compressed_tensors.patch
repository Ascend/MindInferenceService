--- /opt/vllm-ascend/vllm/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py	2025-05-26 16:39:18.000000000 +0000
+++ compressed_tensors.py	2025-05-26 14:25:38.696000000 +0000
@@ -4,6 +4,7 @@
 from typing import Any, Dict, List, Literal, Optional, Tuple, cast
 
 import torch
+import torch_npu
 from compressed_tensors.config import (CompressionFormat,
                                        SparsityCompressionConfig,
                                        SparsityStructure)
@@ -16,6 +17,7 @@
 from vllm.model_executor.layers.fused_moe import FusedMoE
 from vllm.model_executor.layers.linear import (LinearBase, LinearMethodBase,
                                                UnquantizedLinearMethod)
+from vllm.model_executor.layers.quantization import register_quantization_config
 from vllm.model_executor.layers.quantization.base_config import (  # noqa: E501
     QuantizationConfig, QuantizeMethodBase)
 from vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors_moe import (  # noqa: E501
@@ -37,8 +39,10 @@
 
 SPARSITY_CONFIG_NAME: Literal["sparsity_config"] = "sparsity_config"
 QUANTIZATION_SCHEME_MAP_TYPE = Dict[str, Optional[Dict[str, QuantizationArgs]]]
+QUANTIZATION_TYPE = "ascend"
 
 
+@register_quantization_config(QUANTIZATION_TYPE)
 class CompressedTensorsConfig(QuantizationConfig):
 
     def __init__(
@@ -61,6 +65,9 @@
         self.sparsity_ignore_list = sparsity_ignore_list
         self.config = config
 
+    def __repr__(self) -> str:
+        return "AscendQuantConfigCompressedTensor: \n" + super().__repr__()
+
     def get_linear_method(self) -> "CompressedTensorsLinearMethod":
         return CompressedTensorsLinearMethod(self)
 
@@ -72,7 +79,7 @@
         return 70
 
     def get_name(self) -> str:
-        return "compressed-tensors"
+        return QUANTIZATION_TYPE
 
     def get_quant_method(
         self,
@@ -100,6 +107,14 @@
         return None
 
     @classmethod
+    def override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]:
+        try:
+            import torch_npu
+            return QUANTIZATION_TYPE
+        except:
+            return None
+
+    @classmethod
     def from_config(cls, config: Dict[str, Any]) -> "CompressedTensorsConfig":
         ignore: List[str] = cast(List[str], config.get("ignore", []))
         quant_format = cast(str, config.get("format"))
@@ -622,3 +637,4 @@
                 "Only support symmetric scaling factor "
                 "for compressed-tensors KV cache. "
                 f"However found symmetric: {is_symmetric}")
+
