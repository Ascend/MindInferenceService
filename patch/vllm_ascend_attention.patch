--- /usr/local/lib/python3.10/dist-packages/vllm_ascend/attention/attention.py	2025-05-26 16:43:25.000000000 +0000
+++ vllm_ascend_attention.py	2025-05-27 12:31:52.176000000 +0000
@@ -732,7 +732,7 @@
                 self.key_cache, self.value_cache = kv_cache[0], kv_cache[1]
             slots = attn_metadata.slot_mapping
 
-        if hasattr(layer, 'quant_method'):
+        if hasattr(layer, 'quant_method') and False: # Attention weight quantization is not supported yet.
             isPrefill = True if attn_metadata.num_prefills > 0 else False
             if isPrefill:
                 assert attn_metadata.prefill_metadata is not None
@@ -1183,3 +1183,4 @@
         output, _ = self.o_proj(attn_output.reshape(num_tokens, -1))
 
         return output
+
